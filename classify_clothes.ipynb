{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the CNN model for the Fasion-MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#For resetting the graph\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /cxldata/datasets/project/fashion-mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /cxldata/datasets/project/fashion-mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /cxldata/datasets/project/fashion-mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /cxldata/datasets/project/fashion-mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "fashion_mnist = input_data.read_data_sets(\"/datasets/project/fashion-mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = fashion_mnist.train.images\n",
    "X_test = fashion_mnist.test.images\n",
    "y_train = fashion_mnist.train.labels.astype(\"int\")\n",
    "y_test = fashion_mnist.test.labels.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 784), (10000, 784))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8954aafb38>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[1].reshape(28, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us try first using the very simple approach without any optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28*28\n",
    "n_hidden1 = 150\n",
    "n_hidden2 = 150\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.72 Val accuracy: 0.8196\n",
      "1 Train accuracy: 0.84 Val accuracy: 0.8348\n",
      "2 Train accuracy: 0.9 Val accuracy: 0.8366\n",
      "3 Train accuracy: 0.82 Val accuracy: 0.8522\n",
      "4 Train accuracy: 0.8 Val accuracy: 0.852\n",
      "5 Train accuracy: 0.9 Val accuracy: 0.8484\n",
      "6 Train accuracy: 0.88 Val accuracy: 0.866\n",
      "7 Train accuracy: 0.82 Val accuracy: 0.8664\n",
      "8 Train accuracy: 0.82 Val accuracy: 0.866\n",
      "9 Train accuracy: 0.92 Val accuracy: 0.8714\n",
      "10 Train accuracy: 0.92 Val accuracy: 0.867\n",
      "11 Train accuracy: 0.96 Val accuracy: 0.8738\n",
      "12 Train accuracy: 0.86 Val accuracy: 0.8732\n",
      "13 Train accuracy: 0.96 Val accuracy: 0.8758\n",
      "14 Train accuracy: 0.88 Val accuracy: 0.8762\n",
      "15 Train accuracy: 0.92 Val accuracy: 0.8754\n",
      "16 Train accuracy: 0.92 Val accuracy: 0.881\n",
      "17 Train accuracy: 0.96 Val accuracy: 0.877\n",
      "18 Train accuracy: 0.96 Val accuracy: 0.8772\n",
      "19 Train accuracy: 0.94 Val accuracy: 0.8828\n",
      "20 Train accuracy: 0.96 Val accuracy: 0.882\n",
      "21 Train accuracy: 0.92 Val accuracy: 0.8824\n",
      "22 Train accuracy: 0.9 Val accuracy: 0.8796\n",
      "23 Train accuracy: 0.92 Val accuracy: 0.887\n",
      "24 Train accuracy: 0.92 Val accuracy: 0.8832\n",
      "25 Train accuracy: 0.88 Val accuracy: 0.8824\n",
      "26 Train accuracy: 0.96 Val accuracy: 0.8856\n",
      "27 Train accuracy: 0.82 Val accuracy: 0.8842\n",
      "28 Train accuracy: 0.94 Val accuracy: 0.8854\n",
      "29 Train accuracy: 0.94 Val accuracy: 0.8888\n",
      "30 Train accuracy: 0.94 Val accuracy: 0.8858\n",
      "31 Train accuracy: 0.98 Val accuracy: 0.8884\n",
      "32 Train accuracy: 0.92 Val accuracy: 0.889\n",
      "33 Train accuracy: 0.96 Val accuracy: 0.8872\n",
      "34 Train accuracy: 0.98 Val accuracy: 0.8894\n",
      "35 Train accuracy: 0.94 Val accuracy: 0.8882\n",
      "36 Train accuracy: 0.94 Val accuracy: 0.8906\n",
      "37 Train accuracy: 0.9 Val accuracy: 0.8894\n",
      "38 Train accuracy: 0.9 Val accuracy: 0.8876\n",
      "39 Train accuracy: 0.96 Val accuracy: 0.8876\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(fashion_mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = fashion_mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: fashion_mnist.validation.images,\n",
    "                                            y: fashion_mnist.validation.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckpts/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we did not get a good accuracy that means we have to tweek somewhat into our code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us use Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Batch Normalization\n",
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "\n",
    "\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "from functools import partial\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.781\n",
      "1 Test accuracy: 0.8049\n",
      "2 Test accuracy: 0.8165\n",
      "3 Test accuracy: 0.8269\n",
      "4 Test accuracy: 0.8308\n",
      "5 Test accuracy: 0.8363\n",
      "6 Test accuracy: 0.8407\n",
      "7 Test accuracy: 0.8453\n",
      "8 Test accuracy: 0.8445\n",
      "9 Test accuracy: 0.8483\n",
      "10 Test accuracy: 0.8495\n",
      "11 Test accuracy: 0.8535\n",
      "12 Test accuracy: 0.8528\n",
      "13 Test accuracy: 0.8558\n",
      "14 Test accuracy: 0.855\n",
      "15 Test accuracy: 0.8586\n",
      "16 Test accuracy: 0.8591\n",
      "17 Test accuracy: 0.8598\n",
      "18 Test accuracy: 0.8625\n",
      "19 Test accuracy: 0.8596\n",
      "20 Test accuracy: 0.8641\n",
      "21 Test accuracy: 0.8646\n",
      "22 Test accuracy: 0.8665\n",
      "23 Test accuracy: 0.8624\n",
      "24 Test accuracy: 0.8615\n",
      "25 Test accuracy: 0.8683\n",
      "26 Test accuracy: 0.8659\n",
      "27 Test accuracy: 0.8694\n",
      "28 Test accuracy: 0.8697\n",
      "29 Test accuracy: 0.8585\n",
      "30 Test accuracy: 0.8687\n",
      "31 Test accuracy: 0.869\n",
      "32 Test accuracy: 0.8709\n",
      "33 Test accuracy: 0.8668\n",
      "34 Test accuracy: 0.8684\n",
      "35 Test accuracy: 0.8707\n",
      "36 Test accuracy: 0.873\n",
      "37 Test accuracy: 0.8732\n",
      "38 Test accuracy: 0.8714\n",
      "39 Test accuracy: 0.8731\n",
      "40 Test accuracy: 0.8735\n",
      "41 Test accuracy: 0.8678\n",
      "42 Test accuracy: 0.8705\n",
      "43 Test accuracy: 0.8731\n",
      "44 Test accuracy: 0.874\n",
      "45 Test accuracy: 0.87\n",
      "46 Test accuracy: 0.871\n",
      "47 Test accuracy: 0.8769\n",
      "48 Test accuracy: 0.8758\n",
      "49 Test accuracy: 0.8763\n",
      "50 Test accuracy: 0.8716\n",
      "51 Test accuracy: 0.8718\n",
      "52 Test accuracy: 0.8741\n",
      "53 Test accuracy: 0.8759\n",
      "54 Test accuracy: 0.8789\n",
      "55 Test accuracy: 0.8734\n",
      "56 Test accuracy: 0.8691\n",
      "57 Test accuracy: 0.8777\n",
      "58 Test accuracy: 0.8714\n",
      "59 Test accuracy: 0.8747\n",
      "60 Test accuracy: 0.8718\n",
      "61 Test accuracy: 0.8727\n",
      "62 Test accuracy: 0.8785\n",
      "63 Test accuracy: 0.8716\n",
      "64 Test accuracy: 0.8785\n",
      "65 Test accuracy: 0.8731\n",
      "66 Test accuracy: 0.8781\n",
      "67 Test accuracy: 0.877\n",
      "68 Test accuracy: 0.8726\n",
      "69 Test accuracy: 0.8748\n",
      "70 Test accuracy: 0.8786\n",
      "71 Test accuracy: 0.8785\n",
      "72 Test accuracy: 0.8736\n",
      "73 Test accuracy: 0.8749\n",
      "74 Test accuracy: 0.8796\n",
      "75 Test accuracy: 0.8794\n",
      "76 Test accuracy: 0.8779\n",
      "77 Test accuracy: 0.8765\n",
      "78 Test accuracy: 0.8784\n",
      "79 Test accuracy: 0.8718\n",
      "80 Test accuracy: 0.8585\n",
      "81 Test accuracy: 0.8806\n",
      "82 Test accuracy: 0.8726\n",
      "83 Test accuracy: 0.8753\n",
      "84 Test accuracy: 0.8774\n",
      "85 Test accuracy: 0.8735\n",
      "86 Test accuracy: 0.8774\n",
      "87 Test accuracy: 0.8773\n",
      "88 Test accuracy: 0.8808\n",
      "89 Test accuracy: 0.872\n",
      "90 Test accuracy: 0.8717\n",
      "91 Test accuracy: 0.8732\n",
      "92 Test accuracy: 0.8624\n",
      "93 Test accuracy: 0.8768\n",
      "94 Test accuracy: 0.8794\n",
      "95 Test accuracy: 0.874\n",
      "96 Test accuracy: 0.8725\n",
      "97 Test accuracy: 0.8798\n",
      "98 Test accuracy: 0.8797\n",
      "99 Test accuracy: 0.8804\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(fashion_mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = fashion_mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: fashion_mnist.test.images,\n",
    "                                                y: fashion_mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckpts/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here also we are getting a bad accuracy that means further we have to do more optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are usinhg Gradient clipping what it does is that it is used to prevent Vanishing/Exploding Gradient problem more better as compared to the batch and normal of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient clipping\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  \n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.3125\n",
      "1 Test accuracy: 0.7945\n",
      "2 Test accuracy: 0.8804\n",
      "3 Test accuracy: 0.9025\n",
      "4 Test accuracy: 0.911\n",
      "5 Test accuracy: 0.9198\n",
      "6 Test accuracy: 0.9257\n",
      "7 Test accuracy: 0.9306\n",
      "8 Test accuracy: 0.9343\n",
      "9 Test accuracy: 0.9381\n",
      "10 Test accuracy: 0.9409\n",
      "11 Test accuracy: 0.9446\n",
      "12 Test accuracy: 0.9493\n",
      "13 Test accuracy: 0.9503\n",
      "14 Test accuracy: 0.9499\n",
      "15 Test accuracy: 0.9533\n",
      "16 Test accuracy: 0.9545\n",
      "17 Test accuracy: 0.9556\n",
      "18 Test accuracy: 0.9589\n",
      "19 Test accuracy: 0.9587\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckpts/my_model_final.ckpt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see that only in 20 epochs our accuracy is improved so much and it is above 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can further improve its accuracy by using several Fast Optimization so lets us use Adam Optimiser as it usually gives good accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28 \n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.9591\n",
      "1 Test accuracy: 0.9596\n",
      "2 Test accuracy: 0.9673\n",
      "3 Test accuracy: 0.9693\n",
      "4 Test accuracy: 0.9701\n",
      "5 Test accuracy: 0.9696\n",
      "6 Test accuracy: 0.9722\n",
      "7 Test accuracy: 0.9727\n",
      "8 Test accuracy: 0.976\n",
      "9 Test accuracy: 0.973\n",
      "10 Test accuracy: 0.9742\n",
      "11 Test accuracy: 0.9755\n",
      "12 Test accuracy: 0.9761\n",
      "13 Test accuracy: 0.9767\n",
      "14 Test accuracy: 0.9717\n",
      "15 Test accuracy: 0.9725\n",
      "16 Test accuracy: 0.9753\n",
      "17 Test accuracy: 0.9731\n",
      "18 Test accuracy: 0.9786\n",
      "19 Test accuracy: 0.9753\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckpts/my_model_final.ckpt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we can see we have achieved a quite good accuracy with just tweeking our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
